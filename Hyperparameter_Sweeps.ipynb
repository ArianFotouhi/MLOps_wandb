{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiGXlao-mEfn"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W&B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{sweeps-video} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-oPAQ71mEft"
   },
   "outputs": [],
   "source": [
    "!pip install wandb -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzN_N6dqmEfu"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbRXvUmhmEfu"
   },
   "source": [
    "# Step 1ï¸âƒ£. Define the Sweep\n",
    "\n",
    "Fundamentally, a Sweep combines a strategy for trying out a bunch of hyperparameter values with the code that evalutes them.\n",
    "Whether that strategy is as simple as trying every option\n",
    "or as complex as [BOHB](https://arxiv.org/abs/1807.01774),\n",
    "Weights & Biases Sweeps have you covered.\n",
    "You just need to _define your strategy_\n",
    "in the form of a [configuration](https://docs.wandb.com/sweeps/configuration).\n",
    "\n",
    "When you're setting up a Sweep in a notebook like this,\n",
    "that config object is a nested dictionary.\n",
    "When you run a Sweep via the command line,\n",
    "the config object is a\n",
    "[YAML file](https://docs.wandb.com/sweeps/quickstart#2-sweep-config).\n",
    "\n",
    "Let's walk through the definition of a Sweep config together.\n",
    "We'll do it slowly, so we get a chance to explain each component.\n",
    "In a typical Sweep pipeline,\n",
    "this step would be done in a single assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZwZ0hqkmEfu"
   },
   "source": [
    "### ðŸ‘ˆ Pick a `method`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDWFJTn3mEfv"
   },
   "source": [
    "The first thing we need to define is the `method`\n",
    "for choosing new parameter values.\n",
    "\n",
    "We provide the following search `methods`:\n",
    "*   **`grid` Search** â€“ Iterate over every combination of hyperparameter values.\n",
    "Very effective, but can be computationally costly.\n",
    "*   **`random` Search** â€“ Select each new combination at random according to provided `distribution`s. Surprisingly effective!\n",
    "*   **`bayes`ian Search** â€“ Create a probabilistic model of metric score as a function of the hyperparameters, and choose parameters with high probability of improving the metric. Works well for small numbers of continuous parameters but scales poorly.\n",
    "\n",
    "We'll stick with `random`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or7zfYjYmEfv"
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEZH0iiZmEfw"
   },
   "source": [
    "For `bayes`ian Sweeps,\n",
    "you also need to tell us a bit about your `metric`.\n",
    "We need to know its `name`, so we can find it in the model outputs\n",
    "and we need to know whether your `goal` is to `minimize` it\n",
    "(e.g. if it's the squared error)\n",
    "or to `maximize` it\n",
    "(e.g. if it's the accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrzJxjdLmEfw"
   },
   "outputs": [],
   "source": [
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'\n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWvoFrPVmEfw"
   },
   "source": [
    "If you're not running a `bayes`ian Sweep, you don't have to,\n",
    "but it's not a bad idea to include this in your `sweep_config` anyway,\n",
    "in case you change your mind later.\n",
    "It's also good reproducibility practice to keep note of things like this,\n",
    "in case you, or someone else,\n",
    "come back to your Sweep in 6 months or 6 years\n",
    "and don't know whether `val_G_batch` is supposed to be high or low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqbNB5PWmEfw"
   },
   "source": [
    "### ðŸ“ƒ Name the hyper`parameters`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvXj-y15mEfw"
   },
   "source": [
    "Once you've picked a `method` to try out new values of the hyperparameters,\n",
    "you need to define what those `parameters` are.\n",
    "\n",
    "Most of the time, this step is straightforward:\n",
    "you just give the `parameter` a name\n",
    "and specify a list of legal `values`\n",
    "of the parameter.\n",
    "\n",
    "For example, when we choose the `optimizer` for our network,\n",
    "there's only a finite number of options.\n",
    "Here we stick with the two most popular choices, `adam` and `sgd`.\n",
    "Even for hyperparameters that have potentially infinite options,\n",
    "it usually only makes sense to try out\n",
    "a few select `values`,\n",
    "as we do here with the hidden `layer_size` and `dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxvUF3UBmEfw"
   },
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']\n",
    "        },\n",
    "    'fc_layer_size': {\n",
    "        'values': [128, 256, 512]\n",
    "        },\n",
    "    'dropout': {\n",
    "          'values': [0.3, 0.4, 0.5]\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjMFeQCVmEfx"
   },
   "source": [
    "It's often the case that there are hyperparameters\n",
    "that we don't want to vary in this Sweep,\n",
    "but which we still want to set in our `sweep_config`.\n",
    "\n",
    "In that case, we just set the `value` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0DpIQjOmEfx"
   },
   "outputs": [],
   "source": [
    "parameters_dict.update({\n",
    "    'epochs': {\n",
    "        'value': 1}\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocwyETT_mEfx"
   },
   "source": [
    "For a `grid` search, that's all you ever need.\n",
    "\n",
    "For a `random` search,\n",
    "all the `values` of a parameter are equally likely to be chosen on a given run.\n",
    "\n",
    "If that just won't do,\n",
    "you can instead specify a named `distribution`,\n",
    "plus its parameters, like the mean `mu`\n",
    "and standard deviation `sigma` of a `normal` distribution.\n",
    "\n",
    "See more on how to set the distributions of your random variables [here](https://docs.wandb.com/sweeps/configuration#distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCZe3FzmmEfx"
   },
   "outputs": [],
   "source": [
    "parameters_dict.update({\n",
    "    'learning_rate': {\n",
    "        # a uniform distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0,\n",
    "        'max': 0.1\n",
    "      },\n",
    "    'batch_size': {\n",
    "        # integers between 32 and 256\n",
    "        # with evenly-distributed logarithms\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q': 8,\n",
    "        'min': 32,\n",
    "        'max': 256,\n",
    "      }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkjJOinZmEfx"
   },
   "source": [
    "When we're finished, `sweep_config` is a nested dictionary\n",
    "that specifies exactly which `parameters` we're interested in trying\n",
    "and what `method` we're going to use to try them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aj2k5zU9mEfy"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmhkfO0MmEfy"
   },
   "source": [
    "But that's not all of the configuration options!\n",
    "\n",
    "For example, we also offer the option to `early_terminate` your runs with the [HyperBand](https://arxiv.org/pdf/1603.06560.pdf) scheduling algorithm. See more [here](https://docs.wandb.com/sweeps/configuration#stopping-criteria).\n",
    "\n",
    "You can find a list of all configuration options [here](https://docs.wandb.com/library/sweeps/configuration)\n",
    "and a big collection of examples in YAML format [here](https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWj34wcxmEfy"
   },
   "source": [
    "# Step 2ï¸âƒ£. Initialize the Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NA6POYiWmEfy"
   },
   "source": [
    "Once you've defined the search strategy, it's time to set up something to implement it.\n",
    "\n",
    "The clockwork taskmaster in charge of our Sweep is known as the _Sweep Controller_.\n",
    "As each run completes, it will issue a new set of instructions\n",
    "describing a new run to execute.\n",
    "These instructions are picked up by _agents_\n",
    "who actually perform the runs.\n",
    "\n",
    "In a typical Sweep, the Controller lives on _our_ machine,\n",
    "while the agents who complete runs live on _your_ machine(s),\n",
    "like in the diagram below.\n",
    "This division of labor makes it super easy to scale up Sweeps\n",
    "by just adding more machines to run agents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkhGqywCmEfy"
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"pytorch-sweeps-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf212F2CmEfy"
   },
   "source": [
    "# Step 3ï¸âƒ£. Run the Sweep agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jZ7cE0EmEfy"
   },
   "source": [
    "### ðŸ’» Define Your Training Procedure\n",
    "\n",
    "Before we can actually execute the sweep,\n",
    "we need to define the training procedure that uses those values.\n",
    "\n",
    "In the functions below, we define a simple fully-connected neural network in PyTorch, and add the following `wandb` tools to log model metrics, visualize performance and output and track our experiments:\n",
    "* [**`wandb.init()`**](https://docs.wandb.com/library/init) â€“ Initialize a new W&B Run. Each Run is a single execution of the training function.\n",
    "* [**`wandb.config`**](https://docs.wandb.com/library/config) â€“ Save all your hyperparameters in a configuration object so they can be logged. Read more about how to use `wandb.config` [here](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb).\n",
    "* [**`wandb.log()`**](https://docs.wandb.com/library/log) â€“ log model behavior to W&B. Here, we just log the performance; see [this Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Log_(Almost)_Anything_with_W%26B_Media.ipynb) for all the other rich media that can be logged with `wandb.log`.\n",
    "\n",
    "For more details on instrumenting W&B with PyTorch, see [this Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESXq7lojmEfz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        loader = build_dataset(config.batch_size)\n",
    "        network = build_network(config.fc_layer_size, config.dropout)\n",
    "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_loss = train_epoch(network, loader, optimizer)\n",
    "            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ppcp0xD5mEfz"
   },
   "source": [
    "This cell defines the four pieces of our training procedure:\n",
    "`build_dataset`, `build_network`, `build_optimizer`, and `train_epoch`.\n",
    "\n",
    "All of these are a standard part of a basic PyTorch pipeline,\n",
    "and their implementation is unaffected by the use of W&B,\n",
    "so we won't comment on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOdWffRAmEfz"
   },
   "outputs": [],
   "source": [
    "def build_dataset(batch_size):\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    # download MNIST training dataset\n",
    "    dataset = datasets.MNIST(\".\", train=True, download=True,\n",
    "                             transform=transform)\n",
    "    sub_dataset = torch.utils.data.Subset(\n",
    "        dataset, indices=range(0, len(dataset), 5))\n",
    "    loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def build_network(fc_layer_size, dropout):\n",
    "    network = nn.Sequential(  # fully-connected, single hidden layer\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(784, fc_layer_size), nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(fc_layer_size, 10),\n",
    "        nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return network.to(device)\n",
    "\n",
    "\n",
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_epoch(network, loader, optimizer):\n",
    "    cumu_loss = 0\n",
    "    for _, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # âž¡ Forward pass\n",
    "        loss = F.nll_loss(network(data), target)\n",
    "        cumu_loss += loss.item()\n",
    "\n",
    "        # â¬… Backward pass + weight update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\"batch loss\": loss.item()})\n",
    "\n",
    "    return cumu_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfnzKQkHmEfz"
   },
   "source": [
    "Now, we're ready to start sweeping! ðŸ§¹ðŸ§¹ðŸ§¹\n",
    "\n",
    "Sweep Controllers, like the one we made by running `wandb.sweep`,\n",
    "sit waiting for someone to ask them for a `config` to try out.\n",
    "\n",
    "That someone is an `agent`, and they are created with `wandb.agent`.\n",
    "To get going, the agent just needs to know\n",
    "1. which Sweep it's a part of (`sweep_id`)\n",
    "2. which function it's supposed to run (here, `train`)\n",
    "3. (optionally) how many configs to ask the Controller for (`count`)\n",
    "\n",
    "FYI, you can start multiple `agent`s with the same `sweep_id`\n",
    "on different compute resources,\n",
    "and the Controller will ensure that they work together\n",
    "according to the strategy laid out in the `sweep_config`.\n",
    "This makes it trivially easy to scale your Sweeps across as many nodes as you can get ahold of!\n",
    "\n",
    "> _Side Note:_ on the command line, this function is replaced with\n",
    "```\n",
    "wandb agent sweep_id\n",
    "```\n",
    "[Learn more about using Sweeps in the command line âž¡](https://docs.wandb.com/sweeps/quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6KsZbzWmEfz"
   },
   "source": [
    "The cell below will launch an `agent` that runs `train` 5 times,\n",
    "usingly the randomly-generated hyperparameter values returned by the Sweep Controller. Execution takes under 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StjPxgsPmEfz"
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_i01b3N6yix"
   },
   "source": [
    "<img src=\"./asset/result1.jpg\" alt=\"result1\">\n",
    "<img src=\"./asset/result2.jpg\" alt=\"result2\">"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
